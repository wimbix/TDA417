Running the slow program
------------------------

Q: What is the complexity of findSimilarity?
Answer in terms of N, the total number of 5-grams in the input files.
Assume that the number of 5-grams that occur in more than one file is a small
constant - that is, there is not much plagiarised text.

A: O(n^2)

Q: How long did the program take on the 'small' and 'medium' directories?
Is the ratio between the times what you would expect, given the complexity?
Explain very briefly why.

A:  Small: 2.19s
    Medium: 359.03s

    Medium has about 10 times more 5-grams than small, if we look at the complexity we can assume that it would tak about 100 times longer.
    And given your tip, its roughly 2 times slower than that

Q: How long do you predict the program would take to run on the 'huge'
directory?

A: The huge set has about 20 times as many 5-grams, which with our complexity would be about 400 times slower.
About 40h, and given your tip about 80h


Using binary search trees
-------------------------

Q: Which of the trees usually become unbalanced?

A: Files

Q (optional): Is there a simple way to stop these trees becoming unbalanced?

A (optional): 

Using scapegoat trees
---------------------

Q: Now what is the total complexity of buildIndex and findSimilarity?
Again, assume a total of N 5-grams, and a constant number of 5-grams that
occur in more than one file.

A:
buildIndex: O(n log n)
findSimilayity: O(n log n)
Total: O(n log n)

Q (optional): What if the total similarity score is an arbitrary number S,
rather than a small constant?

A (optional):

Q (optional): Did you find any text that was clearly copied?

A (optional):
